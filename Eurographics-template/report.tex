% ---------------------------------------------------------------------------
% Author guideline and sample document for EG publication using LaTeX2e input
% D.Fellner, v2, June 1, 2017

\documentclass{egpubl}

% --- for  Annual CONFERENCE
% \ConferenceSubmission % uncomment for Conference submission
% \ConferencePaper      % uncomment for (final) Conference Paper
% \STAR                 % uncomment for STAR contribution
% \Tutorial             % uncomment for Tutorial contribution
% \ShortPresentation    % uncomment for (final) Short Conference Presentation
%
% --- for  CGF Journal
\JournalSubmission    % uncomment for submission to Computer Graphics Forum
%  \JournalPaper         % uncomment for final version of Journal Paper (NOTE: this won't have page numbers)
%
% --- for  CGF Journal: special issue
% \SpecialIssueSubmission    % uncomment for submission to Computer Graphics Forum, special issue
% \SpecialIssuePaper         % uncomment for final version of Journal Paper, special issue
%
% --- for  EG Workshop Proceedings
% \WsSubmission    % uncomment for submission to EG Workshop
% \WsPaper         % uncomment for final version of EG Workshop contribution
%
 \electronicVersion % can be used both for the printed and electronic version

% !! *please* don't change anything above
% !! unless you REALLY know what you are doing
% ------------------------------------------------------------------------

% for including postscript figures
% mind: package option 'draft' will replace PS figure by a filname within a frame
\ifpdf \usepackage[pdftex]{graphicx} \pdfcompresslevel=9
\else \usepackage[dvips]{graphicx} \fi

\PrintedOrElectronic

% prepare for electronic version of your document
\usepackage{t1enc,dfadobe}

\usepackage{egweblnk}
\usepackage{cite}

% For backwards compatibility to old LaTeX type font selection.
% Uncomment if your document adheres to LaTeX2e recommendations.
% \let\rm=\rmfamily    \let\sf=\sffamily    \let\tt=\ttfamily
% \let\it=\itshape     \let\sl=\slshape     \let\sc=\scshape
% \let\bf=\bfseries

% end of prologue

% \input{EGauthorGuidelines-body.inc}

\title[Polygonal Model Compression]%
      {Polygonal Model Compression with Graph Symmetries}

% for anonymous conference submission please enter your SUBMISSION ID
% instead of the author's name (and leave the affiliation blank) !!
\author[Andrej Jočić]
{\parbox{\textwidth}{\centering% D.\,W. Fellner\thanks{Chairman Eurographics Publications Board}$^{1,2}$
        Andrej Jočić$^{1}$ 
%        S. Spencer$^2$\thanks{Chairman Siggraph Publications Board}
        }
        \\
% For Computer Graphics Forum: Please use the abbreviation of your first name.
{\parbox{\textwidth}{\centering %$^1$TU Darmstadt \& Fraunhofer IGD, Germany\\
         $^1$University of Ljubljana, Faculty of Computer and Information Science, Slovenia
%        $^2$ Another Department to illustrate the use in papers from authors
%             with different affiliations
       }
}
}
% ------------------------------------------------------------------------

% if the Editors-in-Chief have given you the data, you may uncomment
% the following five lines and insert it here
%
% \volume{36}   % the volume in which the issue will be published;
% \issue{1}     % the issue number of the publication
% \pStartPage{1}      % set starting page


%-------------------------------------------------------------------------
\begin{document}

% uncomment for using teaser
% \teaser{
%  \includegraphics[width=\linewidth]{eg_new}
%  \centering
%   \caption{New EG Logo}
% \label{fig:teaser}
%}

\maketitle
%-------------------------------------------------------------------------
\begin{abstract}

TODO the abstract
%-------------------------------------------------------------------------
%  ACM CCS 1998
%  (see http://www.acm.org/about/class/1998)
% \begin{classification} % according to http:http://www.acm.org/about/class/1998
% \CCScat{Computer Graphics}{I.3.3}{Picture/Image Generation}{Line and curve generation}
% \end{classification}
%-------------------------------------------------------------------------
%  ACM CCS 2012
   (see http://www.acm.org/about/class/class/2012)
%The tool at \url{http://dl.acm.org/ccs.cfm} can be used to generate
\begin{CCSXML}
    <ccs2012>
    <concept>
    <concept_id>10010147.10010371.10010396.10010397</concept_id>
    <concept_desc>Computing methodologies~Mesh models</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
    <concept>
    <concept_id>10010147.10010371.10010387.10010394</concept_id>
    <concept_desc>Computing methodologies~Graphics file formats</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
    <concept>
    <concept_id>10003752.10003809.10010031.10002975</concept_id>
    <concept_desc>Theory of computation~Data compression</concept_desc>
    <concept_significance>100</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Mesh models}
\ccsdesc[300]{Computing methodologies~Graphics file formats}
\ccsdesc[100]{Theory of computation~Data compression}


\printccsdesc   
\end{abstract}  
%-------------------------------------------------------------------------
\section{Introduction}

Polygonal models are widely used for representing 3D objects in computer graphics. With increased rendering system capabilities and the desire for greater precision, the size of these models has been increasing. To cut down transmission and static storage costs, specialized compression schemes have been developed.
In previously established \cite{meshCompressionSurvey} terms, we will be focusing on lossless single-rate global compression of static polygonal models. This means that we will be considering fixed (non-evolving) data, which will be decompressed it all at once (without progressive transmission at different levels of detail) and in its entirety (not focusing on specific regions of the model as requested by the user).

The information we usually store is the model's geometry (positions of vertices), connectivity/topology (incidence relations among elements), and optionally additional vertex/face attributes (normals, colours, etc.).
Typically the connectivity information takes up the most space (around twice as much as the goemetry information in a triangle mesh homeomorphic to a sphere \cite{rossignac1999edgebreaker}), so it is the most important to compress.
The vertex-to-vertex connectivity relations, also known as the \textit{wireframe} (or the \textit{1-skeleton} of the underlying simplicial comples, to use a term from topology),
can be represented as an undirected graph.
This way we can reduce the mesh compression problem to that of graph compression.
One issue with this approach is that we lose face orientation data, which renderers need for shading and face culling. If a renderer can't infer consistent face orientations from the wireframe alone, we will need to encode this information separately. This is briefly discussed in section \ref{sec:reconstruction}.

Many compression methods focus exclusively on manifold triangle meshes (or another constrained structure) to get the best possible results.  
In this paper we will consider the case of arbitrary polygonal models, even irregular (having faces of different degrees).
Specifically, we will be exploiting symmetries in the graph, which can often occur in the graphical domain.

\section{Mesh File Formats}

A typical mesh file format (e.g. OFF, PLY) encodes the connectivity information as a list of faces, where each face is a list of vertex indices. Within a face we have one vertex index per edge, but usually the majority of edges are shared between two faces (assuming relatively few boundary edges). This means we have about 2 vertex indices per edge, which is a baseline for storing graphs. To do any better, we must encode the connectivity with less than 2 indices per edge, meaning we need to somehow compress the graph. In this paper, we will use local symmetries in the graph to achieve this.

Most commonly used file formats for storing 3D models have the following structure: a header with metadata, then the vertex data, and finally the connectivity data. Connectivity data is usually stored as a list of faces, where each face is a list of vertex indices. This means the graph we will be compressing is the 1-skeleton of the model. Alternatively we could compress the face adjacency graph, but this would just inflate such an input file.

We decided to support compression and decompression of PLY files, since they are widely adopted and relatively simple in terms of structure.
Note that PLY has two formats: compact binary and human-readable ASCII. When reporting compression rates, we will be considering the binary format for a fair comparison with our (binary) compressed files.

If we are willing to give up the vertex information (and infer positions with a layout algorithm), we can just compress the connectivity information. In this case compressing the face incidence graph is a viable option, since we don't have to list the vertices of each face but just an arbitrary identifier. Then we can get the 1-skeleton of the model by computing the dual of the decompressed face incidence graph.

% \subsection{Related Work}
% slap some survey papers in here (graph compression, mesh compression)
% this and that paper has dealt with symmetry-based mesh compression, but to our knowledge it has not been done with automorphisms.

\section{Symmetry Compression} % FIXME: better name (or move your stuff to section "atlas c)

Čibej and Mihelič \cite{cibej2021automorphisms} have developed a general graph compression method based on automorphisms (connectivity-preserving vertex permutations).
A graph $G$ is defined to be \textit{symmetry-compressible} (SC)
if it can be more succinctly represented with a ``residual'' graph $G^\pi$ and an automorphism  $\pi$ of $G$. Here $G^\pi$ is the minimal set of edges required to reconstruct $G$ by applying $\pi$ to its edges until all cycles of the permutation are closed.
Formally, $G \in \mathcal{S}\mathcal{C}$ if $\exists \pi \in Aut(G)$ such that $|G^\pi| + |\pi| < |G|$, where $Aut(G)$ is the group of $G$'s automorphisms and $|\cdot|$ denotes a measure of representation size (e.g. number of edges). 

The relative efficiency of a symmetry-compressed representation is defined as
$\Delta^r(\pi, G) = \frac{|G| - |\pi| - |G^\pi|}{|G|}$.
% Note that for $G \in \mathcal{S}\mathcal{C}$, $\varDelta^r(\pi, G) \in (0, 1)$ and higher values indicate better compression.\varDelta 


The main computational issue in compressing a graph is finding a (compressive) automorphism $\pi$,
and optionally an edge difference $H \oplus G$ that makes finding $\pi$ easier.

The problem is made somewhat easier by the fact that we only need to look for NSC subgraphs. If we replace the subgraph's edges with their NSC representation, we have reduced the representation size of the whole graph. Indeed, if $H \subseteq G$ and $H \in \mathcal{N}\mathcal{S}\mathcal{C}$, then $G \in \mathcal{N}\mathcal{S}\mathcal{C}$.

Unfortunately the most common structure found in polygonal meshes, a triangle, is not SC \cite[Theorem 3]{cibej2021automorphisms}. But we may still find some luck with larger subgraphs.

The paper presents two approaches: graphlet search and bipartite completion.

\subsection{Graphlet Search}
% This one is a lot slower, but may be more appropriate for our case (if we're not compressing on the fly during transmission).
Another approach they present is to search for symmetry-compressible subgraphs. Going through all graphlets up to a certain size (in decreasing order of relative compression efficiency). Then a maximal edge-disjoint set of SC subgraphs is greedily selected to be replaced by their CS representations.

\section{Atlas-based Compression}
We have opted to modify this approach with the help of a \textit{graph atlas} \cite{read1998atlas}, a systematic enumrations of all undirected graphs up to a cerain length. Python's networkx library, which we have chosen for most of our graph data processing, provides access to the 1253 graphlets with $n \leq 7$.
Instead of $(\pi, H^\pi)$ we simply store $H$'s atlas index and a mapping between nodes of $G$ and $H$.
It makes sense in this case because we are only looking for small subgraphs.
Any more than that acn result in unreasonable compression times anyway.
This may be seen as a dictionary-based graph encoding. Since this takes only $1 + n$ integers to store, it is always at least as efficient as the CS representation since an encoding of $\pi$ requires at least $n$ integers.

% TODO: rel. efficency formula, proof that it's compressive for all connected graphlets for n>=4 (AND TRIANGLE!)

This may be seen as a sort of \textit{instancing} on a sub-mesh level. Instancing is a common technique for reducing the description size of a scene with many repeated objects (e.g. foliage elements) where we only store one copy of the object's geometry and associate each instance with its own coordinate transformation. In our case, we're ``storing'' one copy of a graphlet in the atlas and associating each instance with a vertex mapping, analogous to a coordinate transformation.

\subsection{Compressed file format}

Our first approach to serializing the compressed mesh to a binary files was as follows. The header and vertex data are simply copied into its own file. The connectivity data is a series of 4-byte unsigned integers:
\begin{enumerate}
        \item number of edges
        \item each edge, as a pair of vertex indices
        \item number of compressed subgraphs
        \item for each subgraph: its atlas index, followed by $n$ vertex indices (the node mapping).'
\end{enumerate}
Note that we can infer $n$ (the size of the graphlet) from the atlas index, so we don't need to store it explicitly. But this approach may duplicate the same atlas index many times, if graphlets reoccur often in the mesh. Indeed, we have observed very skewed distributions of graphlet frequencies in the meshes we tested.
% TODO: show some data on this
So we also implemented an index multiplicity encoding, which stores (1-3) same as above, but then for each \textit{unique} atlas index, we store the index and the number of times it occurs in the mesh, followed by this many vertex mappings. This yielded notable increase in relative efficiency. % TODO: show some data on this

\section{Mesh Reconstruction} \label{sec:reconstruction}

Decompressing the wireframe is not the end of the story.

When decompressing, we will need to compute the facial walks of the skeleton in order to produce the original face list.

note that enumerating faces can take quite a while. for triangle mesh, complexity is TODO % see nx documentation notes

% TODO: discuss face orientation
Blendar can figure it out, but meshlab can't sem to. see fig. TODO

We did not support face data.

Another small issue is that we can't preserve single-face holes (see TODO), since these can't possibly be inferred from the wireframe alone.

\section{Experiments}

We have tested our implementation on some of the MeshLab sample meshes. They can be downloaded here: \httpAddr{meshlab.net/\#download}.

\section{Near-Symmetry Compression}

We have also explored the second algorithm presented in \cite{cibej2021automorphisms}, which is based on finding near-symmetry compressible subgraphs.

A class of graphs more amenable to practical compression are \textit{near symmetry-compressible} (NSC) graphs. These are ones that can be turned into SC graphs by adding and/or removing a relatively small number of edges.
Formally, $G \in \mathcal{N}\mathcal{S}\mathcal{C}$ if $\exists H, \pi \in Aut(H)$ such that $|H^\pi| + |\pi| + |H \oplus G| < |G|$. Note that $\mathcal{S}\mathcal{C} \subset \mathcal{N}\mathcal{S}\mathcal{C}$.

We can define the relative compression efficiency for a NSC graph similarly to the SC case.

\subsection{Bipartite Completion}

This is the first one we experimented with, because of superior running time.
The idea is to repeatedly find near-complete bipartite subgraphs (which have high relative efficiency) and replace them with their compressed representation. These are found by a greedy optimization starting from each vertex.

To cache the subgraphs we used a priority queue, sorted by relative efficiency. When extracting a sugraph $G(U,V)$, we must invalidate all cache entries that share an edge with it. 
We implemented two caching modes: static and dynamic. In the static mode, we only invalidate cache entries. In the dynamic mode, we also recompute locally optimal bipartite subgraphs from each $x \in U \cup V$. We don't even have to check if they are better than the cached ones, since the cache is already sorted by relative efficiency and the better ones will be extracted first (invalidating the worse ones).

Even the dynamic caching mode (which runs significantly slower) yielded very poor relative efficiency (around 0.1 - 0.15 for the meshes we tested), so we did not pursue this avenue further.

These results may be explained by the fact that typical polygonal meshes planar or locally plane-like (they have euler characteristic close to 2). A complete bipartite graph $K_{n,m}$ with $n \geq 3$ and $m \geq 3$ contains $K_{3,3}$ as a subgraph, so it isn't planar by Wagner's theorem. Thus the only complete bipartite graphs that may often appear in the graphical domain are $K_{1,n}$ (a star) and $K_{2,n}$, but the former%, which may occur with high $n$ in highly tesselated meshes,
is not symmetry-compressible \cite[Theorem 2]{cibej2021automorphisms}. Indeed, $K_{1,n}$ is not even NSC (by \cite[Theorem 6]{cibej2021automorphisms}).
The vast majority of extracted subgraphs we observed for meshes was $K_{2,2}$, which does not have a very good relative efficiency $\Delta^r(K_{2,2}) = \frac{1}{8}$ (note this value falls in the above-reported range of efficiencies).

Of course we could theoretically also find a symmetric difference $G(U,V) \oplus K(U,V)$ to compress an incomplete bipartite subgraph $G(U,V)$, but since (near-)planar graphs are very sparse with $m = O(n)$, this results in too large a $|G(U,V) \oplus K(U,V)|$ for the representation to be compressive. % TODO: prove this is always the case for planar graphs?



\section{Discussion}

% TODO: list connectivity comp. rates from survey table 1

Typically, mesh compression rates are reported in bits per vertex (bpv) or bits per triangle (bpt). We will be reporting our results in bits per edge (bpe), since this is the most relevant measure for graph compression.


\subsection{Future Work}
\begin{itemize}
        \item bit-efficient encoding (instead of pickle dump)
        \item graphlet heuristic sort with eg. orbit counting stats
        \item alternative automorphism search algorithms, suited to the graphical domain
\end{itemize}

\subsection{Conclusions}


%-------------------------------------------------------------------------

%\bibliographystyle{eg-alpha}
\bibliographystyle{eg-alpha-doi}

\bibliography{references}

\end{document}
